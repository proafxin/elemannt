\documentclass[elemannt.tex]{subfile}

\begin{document}
	\chapter{Arithmetic Functions}
	In this chapter, we will discuss some generalized arithmetic functions and their asymptotic behavior. We will skip discussing the basic definitions since they are common in most introductory number theory texts. See \gls{erdos}, \gls{mobius}
		\begin{definition}[Summatory function]
			For an arithmetic function $f$, the \textit{summatory function} of $f$ is defined as
				\begin{align*}
					F(n)
						& = \sum_{d\mid n}f(d)
				\end{align*}
		\end{definition}
	Note that the number of divisor function $\tau(n)$ is the summatory function of the unit function $u(n)=1$. The sum of divisor function $\sigma(n)$ is the summatory function of the invariant function $f(n)=n$. An interesting property that we will repeatedly use is that
		\begin{align*}
			\sum_{i=1}^{n}F(i)
				& = \sum_{i=1}^{n}\sum_{d\mid i}f(d)\\
				& = \sum_{i=1}^{n}\left\lfloor{\dfrac{n}{i}}\right\rfloor f(i)
		\end{align*}
	Here, the last equation is true because there are $\lfloor{n/i}\rfloor$ multiples of $i$ not exceeding $n$. Recall that the number of divisor function $\tau(n)=\sum_{ab=n}1$. We can generalize this as follows.
		\begin{definition}[Generalized number of divisor]
			The \index{Generalized number of divisor}\textit{generalized number of divisor} function is defined as
				\begin{align*}
					\tau_{k}(n)
						& = \sum_{d_{1}\cdots d_{k}=n}1
				\end{align*}
			So $\tau_{k}(n)$ is the number of ways to write $n$ as a product of $k$ positive integers. 
		\end{definition}
	Similarly, we can take the sum of divisor function and generalize it.
		\begin{definition}[Generalized sum of divisor]
			The \index{Generalized sum of divisor}\textit{generalized sum of divisor} function can be defined as
				\begin{align*}
					\sigma_{k}(n)
						& = \sum_{d\mid n}d^{k}
				\end{align*}
		\end{definition}
	At this point, we will discuss some asymptotic notions.
		\begin{definition}[Big O]
			Let $f$ and $g$ be two real or complex valued functions. We say that
				\begin{align*}
					f(x)
					& = O(g(x))
				\end{align*}
			if there is a positive real constant $C$ such that
				\begin{align*}
					|f(x)|
					& \leq Cg(x)
				\end{align*}
			for all sufficiently large $x$. It is also written as $f(x)\ll g(x)$  or $g(x)\gg f(x)$.
		\end{definition}
	In particular, $f(x)=O(1)$ means that $f$ is bounded above by some positive constant. Some trivial examples are $x^{2}=O(x^{3})$, $x+1=O(x)$ and $x^{2}+2x=O(x^{2})$. We usually want $g(x)$ to be as small as possible to avoid triviality. A useful example is
		\begin{align*}
			\left\lfloor{x}\right\rfloor
				& = x+O(1)
		\end{align*}
	since $x=\lfloor{x}\rfloor+\{x\}$ and $0\leq \{x\}<1$.
		\begin{definition}[Small O]
			Let $f$ and $g$ be two real or complex valued functions. Then the following two statements are equivalent
				\begin{align*}
					f(x)
						& = o(g(x))\\
					\iff \lim\limits_{x\to\infty}\dfrac{f(x)}{g(x)}
						& = 0
				\end{align*}
		\end{definition}
	Some trivial examples are $1/x=o(1)$, $x=o(x^{2})$ and $2x^{2}\neq o(x^{2})$.
		\begin{definition}[Equivalence]
			Let $f$ and $g$ be two real or complex valued functions. We say that they are \textit{asymptotically equivalent} if
				\begin{align*}
					\lim\limits_{x\to\infty}\dfrac{f(x)}{g(x)}
						& = 1
				\end{align*}
			and we denote it by $f\sim g$.
		\end{definition}
	An example is $x^{2}\sim x^{2}+x$. Note the following.
		\begin{align*}
			f
				& \sim g\\
			\iff |f(x)-g(x)|
				& = o(g(x))
		\end{align*}
	We will use these symbols extensively throughout the book. It is of utmost importance that the reader gets well familiarized with these notions since they will be crucial in understanding much of this book. The primary motivation behind these asymptotic notions is to get an as precise as possible idea about the \textit{order of magnitude} of a certain function. This is why we will be leaning more towards $x^{2}+2x=O(x^{2})$ than $x^{2}+2x=O(x^{3})$ even though both are mathematically correct. The reason is, even though $x^{2}+2x=O(x^{3})$ is true, it is taking away a great portion of the accuracy to which we suppose $x^{2}+2x$ should be measured with respect to. On the other hand, we easily see that we cannot have $x^{2}+2x=O(x^{\epsilon})$ for $\epsilon<2$. Under the same philosophy, we will be interested in such asymptotic of some functions. Right now it is not quite straightforward why we are so interested in asymptotic estimation. Hopefully, the reason will unveil itself in the coming chapters.
	
	Consider the \textit{harmonic sum}
		\begin{align*}
			H(n)
				& = 1+\dfrac{1}{2}+\ldots+\dfrac{1}{n}
		\end{align*}
	It does not seem easy to calculate $H(n)$ accurately, however, we can make a decent attempt to estimate $H(n)$. The tool that is best suited for carrying out such an estimation is the \textit{Abel partial summation formula}, which today is a cornerstone of analytic number theory.
		\begin{theorem}[Abel partial summation formula]
			Let $\{a_{n}\}$ be a sequence of real numbers and $f$ be a continuously differentiable function in the range $[y,x]$. If the partial sums of $\{a_{n}\}$ is
				\begin{align*}
					A(x)
						& = \sum_{n\leq x}a_{n}
				\end{align*}
			are known, then
				\begin{align*}
					\sum_{y<n\leq x}a_{n}f(n)
						& = A(x)f(x)-A(y)f(y)-\int_{y}^{x}A(t)f'(t)dt
				\end{align*}
		\end{theorem}
	
		\begin{proof}
			
		\end{proof}
	It is not straightforward to realize how such a formula can be as influential as we are describing it to be. Notice that, the formula essentially converts a discreet sum into an integral, which occasionally may be calculable. If the integral is not calculable, we may be able to estimate its value sometimes. We will demonstrate these ideas next. It is worth mentioning that Ramanujan also used such a technique. For example, in \textcite[Page $83$, $\S4$]{aiyangar_hardy_vennkatesvara_seshu_aiyar_p_wilson_1927}, we can definitely see what can only be described as the Abel's partial summation formula. Although, it is very unclear whether Ramanujan simply knew about this or he came up with the idea on his own while working on number of divisors. He essentially derives the partial summation formula while trying to express a sum of the form $\sum_{p\leq x}\phi(p)$ with respect to $\pi(x),\phi(x)$ and an integral where $\pi(x)$ is the number of primes not exceeding $x$.
	
	Now, we are interested in the general number of divisors and general sum of divisors. Let us define the cumulative sum of these functions.
		\begin{align*}
			A_{k}(x)
				& = \sum_{n\leq x}\sigma_{k}(n)\\
			B_{k}(x)
				& = \sum_{n\leq x}\tau_{k}(n)
		\end{align*}
	Notice the following.
		\begin{align*}
			A_{k}(x)
				& = \sum_{n\leq x}\sum_{d\mid n}d^{k}\\
				& = \sum_{n\leq x}\left\lfloor{\dfrac{x}{n}}\right\rfloor n^{k}\\
				& = \sum_{n\leq x}\left(\dfrac{x}{n}+O(1)\right)n^{k}\\
				& = x\sum_{n\leq x}n^{k-1}+O\left(\sum_{n\leq x}n^{k}\right)\\
				& = xA_{k-1}(x)+O\left(\sum_{n\leq x}n^{k}\right)
		\end{align*}
	We can use this to inductively establish an asymptotic for $A_{k}(x)$ if we can establish the asymptotic of $A_{2}(x)$. We will get to that in a moment. First, let us take care of the summation within the big O bracket. We have the trivial inequality that
		\begin{align*}
			\sum_{n\leq x}n^{k}
				& \leq \sum_{n\leq x}x^{k}\\
				& = x^{k}\sum_{n\leq x}1\\
				& = \lfloor{x}\rfloor x^{k}\\
				& = (x+O(1))x^{k}\\
				& = x^{k+1}+O(x^{k})
		\end{align*}
	We have that $A_{k}(x)=xA_{k-1}(x)+O(x^{k+1})$. An interested reader can try and prove that
		\begin{align*}
			(n+1)^{k+1}-1
				& =\sum_{i=0}^{k}\binom{k+1}{i}(1^{i}+\ldots+n^{i})
		\end{align*}
	This is known as the \textit{Pascal identity} (see \textcite{pascal_1964}). For an English translation, see \textcite{knoebel_laubenbacher_lodder_pengelley_2007}.  \textcite{kieren_macmillan_jonathan_sondow_2011} may also be of interest.
\end{document}