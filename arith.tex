\documentclass[elemannt.tex]{subfile}

\begin{document}
	\chapter{Arithmetic Functions}\label{ch:arith}
	In this chapter, we will discuss some generalized arithmetic functions and their asymptotic behavior. The primary reason to analyze asymptotic behavior is to see how a function grows. A very common way to understand the growth of a function $f$ is to find out how the sum
		\begin{align*}
			\sum_{n\leq x}f(n)
		\end{align*}
	behaves as $x\to\infty$. This may seem like a random thing to do at first. But as we will see, analyzing growth of arithmetic functions give us insight to many questions we have. A prominent example of one such question is, how often do the primes occur? We will attempt to answer this question in \autoref{ch:primes}.
	\section{Some Summatory functions}
	We will skip discussing the basic definitions since they are common in most introductory number theory texts.
		\begin{definition}[Summatory function]
			For an arithmetic function $f$, the \index{Summatory function}\textit{summatory function} of $f$ is defined as
				\begin{align*}
					F(n)
						& = \sum_{d\in \mathbb{D}}f(d)
				\end{align*}
			where $\mathbb{D}$ is some set possibly dependent on $n$.
		\end{definition}
	When $\mathbb{D}$ is the set of divisors of $n$, the number of divisor function $\tau(n)$ is the summatory function of the unit function $u(n)=1$ and the sum of divisor function $\sigma(n)$ is the summatory function of the invariant function $f(n)=n$. Another interesting summatory function we will see are functions of the form
		\begin{align*}
			\sum_{n\leq x}f(n)
		\end{align*}
	for a real number $x$. Associated with this is the \textit{average order} of an arithmetic function.
		\begin{align*}
			\lim\limits_{x\to\infty}\dfrac{\sum_{n\leq x}f(x)}{x}
		\end{align*}
	is the average order of the arithmetic function $f$.
	
	An interesting property that we will repeatedly use is that
		\begin{align*}
			\sum_{i=1}^{n}F(i)
				& = \sum_{i=1}^{n}\sum_{d\mid i}f(d)\\
				& = \sum_{i=1}^{n}\left\lfloor{\dfrac{n}{i}}\right\rfloor f(i)
		\end{align*}
	Here, the last equation is true because there are $\lfloor{n/i}\rfloor$ multiples of $i$ not exceeding $n$. Recall that the number of divisor function $\tau(n)=\sum_{ab=n}1$. We can generalize this as follows.
		\begin{definition}[Generalized number of divisor]
			The \index{Generalized number of divisor}\textit{generalized number of divisor} function is defined as
				\begin{align*}
					\tau_{k}(n)
						& = \sum_{d_{1}\cdots d_{k}=n}1
				\end{align*}
			So $\tau_{k}(n)$ is the number of ways to write $n$ as a product of $k$ positive integers. 
		\end{definition}
	Similarly, we can take the sum of divisor function and generalize it.
		\begin{definition}[Generalized sum of divisor]
			The \index{Generalized sum of divisor}\textit{generalized sum of divisor} function can be defined as
				\begin{align*}
					\sigma_{k}(n)
						& = \sum_{d\mid n}d^{k}
				\end{align*}
		\end{definition}
	At this point, we should discuss some asymptotic notions.
		\begin{definition}[Big O]
			Let $f$ and $g$ be two real or complex valued functions. We say that
				\begin{align*}
					f(x)
					& = O(g(x))
				\end{align*}
			if there is a positive real constant $C$ such that
				\begin{align*}
					|f(x)|
					& \leq Cg(x)
				\end{align*}
			for all sufficiently large $x$. It is also written as $f(x)\ll g(x)$  or $g(x)\gg f(x)$. When we say $g$ is an asymptotic estimate of $f$, we mean that
				\begin{align*}
					f(x)
						& = g(x)+O(h(x))
				\end{align*}
			for two functions $g$ and $h$. Here, $h$ is the \textit{error term} which obviously should be of lower magnitude than $g$.
		\end{definition}
	In particular, $f(x)=O(1)$ means that $f$ is bounded above by some positive constant. Some trivial examples are $x^{2}=O(x^{3})$, $x+1=O(x)$ and $x^{2}+2x=O(x^{2})$. We usually want $g(x)$ to be as small as possible to avoid triviality. A useful example is
		\begin{align*}
			\left\lfloor{x}\right\rfloor
				& = x+O(1)
		\end{align*}
	since $x=\lfloor{x}\rfloor+\{x\}$ and $0\leq \{x\}<1$.
		\begin{definition}[Small O]
			Let $f$ and $g$ be two real or complex valued functions. Then the following two statements are equivalent
				\begin{align}
					f(x)
						& = o(g(x))\\
					\lim\limits_{x\to\infty}\dfrac{f(x)}{g(x)}
						& = 0
				\end{align}
		\end{definition}
	Some trivial examples are $1/x=o(1)$, $x=o(x^{2})$ and $2x^{2}\neq o(x^{2})$.  \textcite[Page 883 (second volume is paged consecutively after first volume)]{landau_1909} states that the symbol $O$ had been first used by \textcite[Page 401]{bachmann_1894}. \textcite{hardy_1910} uses the notations $\prec$ and $\succ$ respectively but they are no longer in practice. \textcite{hardy_riesz_1915} adopted the notations small $o$ and big $O$ and today these are the primary notations for this purpose.
	
	It should be evident that having an estimate with respect to $O$ asymptotic formulas is more desirable than $o$ formulas. By nature, $O$ formulas give us a better understanding and a specific estimate whereas $o$ does not always say as much. Moreover, working with $O$ is a lot easier than working with $o$. For example,
		\begin{align*}
			\sum O(f(x))
				& = O\left(\sum f(x)\right)\\
			\int O(f(x))dx
				& = O\left(\int f(x)dx\right)
		\end{align*}
	Or consider the possibility that we can very easily deal with constants that would otherwise pop up here and there unnecessarily. With the help of $O$,
		\begin{align*}
			O(1)+c
				& = O(1)\\
			O(cf(x))
				& = O(f(x))
		\end{align*}
	and so on.
		\begin{definition}[Equivalence]
			Let $f$ and $g$ be two real or complex valued functions. We say that they are \textit{asymptotically equivalent} if
				\begin{align*}
					\lim\limits_{x\to\infty}\dfrac{f(x)}{g(x)}
						& = 1
				\end{align*}
			and we denote it by $f\sim g$. So, we can say that $g$ is an asymptotic formula for $f$.
		\end{definition}
	An example is $x^{2}\sim x^{2}+x$. Note the following.
		\begin{align*}
			f \sim g
				& \iff |f(x)-g(x)|= o(g(x))
		\end{align*}
	We will use these symbols extensively throughout the book. It is of utmost importance that the reader gets well familiarized with these notions since they will be crucial in understanding much of this book. The primary motivation behind these asymptotic notions is to get an as precise as possible idea about the \textit{order of magnitude} of a certain function. This is why we will be leaning more towards $x^{2}+2x=O(x^{2})$ than $x^{2}+2x=O(x^{3})$ even though both are mathematically correct. The reason is, even though $x^{2}+2x=O(x^{3})$ is true, it is taking away a great portion of the accuracy to which we suppose $x^{2}+2x$ should be measured with. On the other hand, we easily see that we cannot have $x^{2}+2x=O(x^{\epsilon})$ for $\epsilon<2$. Under the same philosophy, we define the order of magnitude equivalence.
		\begin{definition}
			If $f$ and $g$ are functions such that both $f(x)\ll g(x)$ and $g(x)\ll f(x)$ hold, then we write $f\asymp g$ and say that $f$ and $g$ have the \textit{same order of magnitude}.
		\end{definition}\par
	Now, we are interested in the general number of divisors and general sum of divisors. Let us define the cumulative sum of these functions.
		\begin{align*}
			S_{k}(x)
				& = \sum_{n\leq x}\sigma_{k}(n)\\
			T_{k}(x)
				& = \sum_{n\leq x}\tau_{k}(n)
		\end{align*}
	Notice the following.
		\begin{align*}
			S_{k}(x)
				& = \sum_{n\leq x}\sum_{d\mid n}d^{k}\\
				& = \sum_{n\leq x}\left\lfloor{\dfrac{x}{n}}\right\rfloor n^{k}\\
				& = \sum_{n\leq x}\left(\dfrac{x}{n}+O(1)\right)n^{k}\\
				& = x\sum_{n\leq x}n^{k-1}+O\left(\sum_{n\leq x}n^{k}\right)
		\end{align*}
	We can use this to establish an asymptotic for $T_{k}(x)$ if we can establish the asymptotic of $A_{2}(x)$. We will get to that in a moment. First, let us take care of the summation within the big O bracket. We have the trivial inequality that
		\begin{align*}
			\sum_{n\leq x}n^{k}
				& \leq \sum_{n\leq x}x^{k}\\
				& = x^{k}\sum_{n\leq x}1\\
				& = \lfloor{x}\rfloor x^{k}\\
				& = (x+O(1))x^{k}\\
				& = x^{k+1}+O(x^{k})
		\end{align*}
	We have that $S_{k}(x)=x\left(x^{k}+O(x^{k-1})\right)+O(x^{k+1})=O(x^{k+1})$. Although weak, we get an estimate this way. On this note, an interested reader can try and prove that
		\begin{align*}
			(n+1)^{k+1}-1
				& =\sum_{i=0}^{k}\binom{k+1}{i}\mathfrak{S}(n,i)
		\end{align*}
	where $\mathfrak{S}(x,k)=\sum_{n\leq x}n^{k}$. This is known as the \index{Pascal identity}\textit{Pascal identity} (see \textcite{pascal_1964}, for an English translation, see \textcite{knoebel_laubenbacher_lodder_pengelley_2007}). \textcite[Chapter II, Theorem $1$]{lehmer_1900} proves that
		\begin{align}
			\mathfrak{S}(x,k)
				& = \dfrac{x^{k+1}}{k+1}+\Delta\label{eqn:lehmers}
		\end{align}
	where $|\Delta|\leq x^{k}$. The reader may also be interested in \textcite{kieren_macmillan_jonathan_sondow_2011}.
	
	We shall try to estimate $T$ in a similar fashion. First, see that
		\begin{align*}
			\tau_{k}(n)
				& = \sum_{d_{1}\cdots d_{k}=n}1\\
				& = \sum_{d_{k}\mid n}\sum_{d_{1}\cdots d_{k-1}=n/d_{k}}1\\
				& = \sum_{d\mid n}\tau_{k-1}\left(\dfrac{n}{d}\right)
		\end{align*}
	Note that the two sets $\{d:d\mid n\}$ and $\{n/d:d\mid n\}$ are actually the same. So, we get
		\begin{align*}
			\tau_{k}(n)
				& = \sum_{d\mid n}\tau_{k-1}(d)
		\end{align*}
	Using this for $T$,
		\begin{align*}
			T_{k}(x)
				& = \sum_{n\leq x}\tau_{k}(n)\\
				& = \sum_{n\leq x}\sum_{d\mid n}\tau_{k-1}(d)\\
				& = \sum_{n\leq x}\left\lfloor{\dfrac{x}{n}}\right\rfloor\tau_{k-1}(n)\\
				& = \sum_{n\leq x}\left(\dfrac{x}{n}+O(1)\right)\tau_{k-1}(n)\\
				& = x\sum_{n\leq x}\dfrac{\tau_{k-1}(n)}{n}+O\left(\sum_{n\leq x}\tau_{k-1}(n)\right)
		\end{align*}
	Thus, we have the recursive result
		\begin{align*}
			T_{k}(x)
				& = x\sum_{n\leq x}\dfrac{\tau_{k-1}(n)}{n}+O(T_{k-1}(x))
		\end{align*}
	It gets nontrivial how to proceed from here. Consider the \textit{harmonic sum}
		\begin{align*}
			H(x)
			& = \sum_{n\leq x}\dfrac{1}{n}
		\end{align*}
	It does not seem easy to calculate $H$ accurately, however, we can make a decent attempt to estimate $H$. The tool that is best suited for carrying out such an estimation is the \index{Abel partial summation formula}\textit{Abel partial summation formula}. \textcite{abel_1826} states this formula which today is a cornerstone of analytic number theory.
		\begin{theorem}[Abel partial summation formula]\label{thm:abel}
			Let $\{a_{n}\}$ be a sequence of real numbers and $f$ be a continuous differentiable function in $[y,x]$. If the partial sums of $\{a_{n}\}$ is
				\begin{align*}
					A(x)
						& = \sum_{n\leq x}a_{n}
				\end{align*}
			are known, then
				\begin{align*}
					\sum_{y<n\leq x}a_{n}f(n)
						& = A(x)f(x)-A(y)f(y)-\int\limits_{y}^{x}A(t)f'(t)dt
				\end{align*}
			In particular, if $f$ is an arithmetic function,
				\begin{align*}
					\sum_{n\leq x}a_{n}f(n)
						& = A(x)f(x)-\int\limits_{1}^{x}A(t)f'(t)dt
				\end{align*}
		\end{theorem}
		
		\begin{proof}
			
		\end{proof}
	It is not straightforward to realize how such a formula can be as influential as we are describing it to be. Notice that, the formula essentially converts a discreet sum into an integral, which occasionally may be calculable. If the integral is not calculable, we may be able to estimate its value sometimes. We will demonstrate these ideas next. It is worth mentioning that Ramanujan also used such a technique. For example, in \textcite[Page $83$, $\S4$]{aiyangar_hardy_vennkatesvara_seshu_aiyar_p_wilson_1927}, we can definitely see what can only be described as the formula itself. It is unclear if Ramanujan simply knew about this. Considering he shows the calculation instead of just mentioning the formula, it is certainly possible he came up with the idea on his own, possibly before he had been working on that particular paper. He essentially derives the partial summation formula while trying to express a sum of the form $\sum_{p\leq x}\phi(p)$ with respect to $\pi(x),\phi(x)$ and an integral where $\pi(x)$ is the number of primes not exceeding $x$. A consequence of \nameref{thm:abel} is the celebrated \index{Euler's summation formula}\textit{Euler's summation formula}.
		\begin{theorem}[Euler's summation formula]\label{thm:eulersum}
			Let $f$ be a continuous differentiable function in $[y,x]$. Then
				\begin{align*}
					\sum_{y<n\leq x}f(n)
						& = \int\limits_{y}^{x}f(t)dt+\int\limits_{y}^{x}\{t\}f(t)dt+\{y\}f(y)-\{x\}f(x)
				\end{align*}
			where ${t}=t-\lfloor{t}\rfloor$ is the fractional part of $t$.
		\end{theorem}
	
		\begin{proof}
			
		\end{proof}
	As an application of \nameref{thm:eulersum}, we can derive a result similar to \ref{eqn:lehmers} taking  $f(n)=n^{k}$ for $k\geq 0$.
		\begin{align*}
			\mathfrak{S}_{k}(x)
				& = \sum_{n\leq x}n^{k}\\
				& = \int\limits_{1}^{x}t^{k}dt+k\int\limits_{1}^{x}t^{k-1}(t-\lfloor{t}\rfloor)dt+1-(x-\lfloor{x}\rfloor)x^{k}\\
				& = \dfrac{x^{k+1}}{k+1}-\dfrac{1}{k+1}+O\left(k\int\limits_{1}^{x}t^{k-1}dt\right)+O(x^{k})\\
				& = \dfrac{x^{k+1}}{k+1}+O(x^{k})
		\end{align*}
	Setting $a_{n}=\tau_{k-1}(n)$ and $f(n)=1/n$ in \nameref{thm:abel}, we get
		\begin{align*}
			\sum_{n\leq x}\dfrac{\tau_{k-1}(n)}{n}
				& = \dfrac{T_{k-1}(x)}{x}-\int\limits_{1}^{x}-\dfrac{T_{k-1}(t)}{t^{2}}dt
		\end{align*}
	Thus, we have a result where we can inductively get to the final expression. First, let us see the case $k=2$.
		\begin{align*}
			\sum_{n\leq x}\tau(n)
				& = \sum_{n\leq x}\left\lfloor{\dfrac{x}{n}}\right\rfloor
		\end{align*}
	Clearly, this is just the number of pairs $(a,b)$ such that $ab\leq x$. We can divide the pairs in two classes. In the first class, $1\leq a\leq \sqrt{x}$ and in the second one, $a>\sqrt{x}$. In the first case, for a fixed $a$, there are $\lfloor{x/a}\rfloor$ possible choices for a valid value of $b$. So, the number of pairs in the first case is
		\begin{align*}
			\sum_{a\leq \sqrt{x}}\left\lfloor{\dfrac{x}{a}}\right\rfloor
		\end{align*}
	In the second case, since $a>\sqrt{x}$ and $b\leq x/a$, we must have $b\leq \sqrt{x}$. For a fixed $b$, there are $\lfloor{x/b}\rfloor-\sqrt{x}$ choices for a valid value of $a$, the choices namely are
		\begin{align*}
			\lfloor{x}\rfloor+1,\ldots,\left\lfloor{\dfrac{x}{b}}\right\rfloor
		\end{align*}
	Then the number of pairs in this case is
		\begin{align*}
			\sum_{b\leq \sqrt{x}}\left\lfloor{\dfrac{x}{b}}\right\rfloor-\sqrt{x}
		\end{align*}
	Thus, the total number of such pairs is
		\begin{align}
			\sum_{a\leq \sqrt{x}}\left\lfloor{\dfrac{x}{a}}\right\rfloor+\sum_{b\leq \sqrt{x}}\left(\left\lfloor{\dfrac{x}{b}}\right\rfloor-\sqrt{x}\right)
				& = 2\sum_{n\leq \sqrt{x}}\left\lfloor{\dfrac{x}{n}}\right\rfloor-\lfloor{x}\rfloor^{2}\label{eqn:totdiv}
		\end{align}
	For getting past this sum, we have to deal with the sum
		\begin{align*}
			\sum_{n\leq \sqrt{x}}\left\lfloor{x/n}\right\rfloor
				& = \sum_{n\leq \sqrt{x}}\left(\dfrac{x}{n}+O(1)\right)\\
				& = x\sum_{n\leq \sqrt{x}}\dfrac{1}{n}+O(\sqrt{x})\\
				& = xH(\sqrt{x})+O(\sqrt{x})
		\end{align*}
	Setting $a_{n}=1$ and $f(n)=1/n$ in \nameref{thm:abel}, we get
		\begin{align*}
			H(x)
				& = \dfrac{A(x)}{x}-\int\limits_{1}^{x}-\dfrac{A(t)}{t^{2}}dt
		\end{align*}
	Here, $A(x)=\lfloor{x}\rfloor=x+O(1)$. Using this,
		\begin{align*}
			H(x)
				& =  1+O\left(\dfrac{1}{x}\right)+\int\limits_{1}^{x}\left(\dfrac{1}{t}+\dfrac{O(1)}{t^{2}}\right)dt\\
				& = 1+O\left(\dfrac{1}{x}\right)+\int\limits_{1}^{x}\dfrac{1}{t}dt+O\left(\int\limits_{1}^{x}\dfrac{1}{t^{2}}dt\right)\\
				& = 1+O\left(\dfrac{1}{x}\right)+\log{x}+O\left(1-\dfrac{1}{x}\right)
		\end{align*}
	Thus, we have the following result.
		\begin{theorem}\label{thm:harmonicsum}
				\begin{align*}
					H(x)
						& = \log{x}+C+O\left(\dfrac{1}{x}\right)
				\end{align*}
			where $C$ is a constant.
		\end{theorem}
	We get a more precise formulation of $H(x)$ by considering the limit $x\to\infty$ which removes $O(1/x)$ from the expression since this limit would be $0$.
		\begin{theorem}
			There is a constant $\gamma$ such that
				\begin{align*}
					\gamma
						& = \lim\limits_{x\to\infty}(H(x)-\log{x})
				\end{align*}
		\end{theorem}
	This constant $\gamma$ is now known as \index{Euler-Mascheroni constant}\textit{Euler's constant} or \textit{Euler-Mascheroni's constant}, although, neither Euler nor Mascheroni used the notation $\gamma$ for this constant. \textcite{euler_1740} (republished in \textcite{euler_2020}) used $C$ and $O$ in his original paper. \textcite{mascheroni_1790} used $A$ and $a$. Today it is not known whether $\gamma$ is even irrational. For now, we will not require the use of $\gamma$, so we will use \autoref{thm:harmonicsum}. Applying this, we have
		\begin{align*}
			\sum_{n\leq \sqrt{x}}\left\lfloor{\dfrac{x}{n}}\right\rfloor
				& = xH(\sqrt{x})+O(\sqrt{x})\\
				& = x\left(C+\log{\sqrt{x}}+O\left(\dfrac{1}{\sqrt{x}}\right)\right)+O(\sqrt{x})\\
				& = \dfrac{1}{2}x\log{x}+Cx+O\left(\dfrac{x}{\sqrt{x}}\right)+O(\sqrt{x})\\
				& = \dfrac{1}{2}x\log{x}+O(x)
		\end{align*}
	We can now use this to get
		\begin{align*}
			\sum_{n\leq x}\tau(n)
				& = 2\sum_{n\leq \sqrt{x}}\left\lfloor{x/n}\right\rfloor-\lfloor{\sqrt{x}}\rfloor^{2}\\
				& = x\log{x}+O(x)
		\end{align*}
	Thus, we get the following result.
		\begin{align*}
			\dfrac{\sum_{n\leq x}\tau(n)}{x}
				& = \log{x}+O(1)
		\end{align*}
	\textcite{dirichlet_1897} actually proves the more precise result given below.
		\begin{theorem}[Dirichlet's average order of $\tau$ theorem]\label{thm:dirtau}
				\begin{align*}
					\dfrac{\sum_{n\leq x}\tau(n)}{x}
						& = \log{x}+2\gamma-1+O\left(\dfrac{1}{\sqrt{x}}\right)
				\end{align*}
			where $\gamma$ is the Euler-Mascheroni constant.
		\end{theorem}
	Then Dirichlet's theorem on $\tau$ can be restated as \textit{the average order of $\tau$ is }$O(\log{x})$. \textcite{aiyangar_hardy_vennkatesvara_seshu_aiyar_p_wilson_1927} points out in his paper that the error term $O(1/\sqrt{x})$ in Dirichlet's theorem can be improved to $O\left(x^{-\frac{2}{3}+\epsilon}\right)$ or $O\left(x^{-2/3}\log{x}\right)$ as \textcite[Page $689$]{landau_1912} shows.
	
	We can now get back to estimating $T$. Using \nameref{thm:abel}, we were able to deduce
		\begin{align*}
			T_{k}(x)
				& = O\left(T_{k-1}(x)\right)+x\int\limits_{1}^{x}\dfrac{T_{k-1}(t)}{t^{2}}dt
		\end{align*}
	Using \nameref{thm:dirtau}, $T(x)=x\log{x}+O(x)$, so
		\begin{align*}
			T_{3}(x)
				& = O(T(x))+x\int\limits_{1}^{x}\dfrac{T(t)}{t^{2}}dt\\
				& = O(x\log{x})+x\int\limits_{1}^{x}\dfrac{\log{t}+O(1)}{t}dt\\
				& = O(x\log{x})+x\int\limits_{1}^{x}\dfrac{\log{t}}{t}dt+xO\left(\int\limits_{1}^{x}\dfrac{1}{t}dt\right)\\
				& = O(x\log{x})+x\int\limits_{1}^{x}\dfrac{\log{t}}{t}dt
		\end{align*}
	Using integration by parts,
		\begin{align*}
			\int\dfrac{\log{t}}{t}dt
				& = \log{t}\int\dfrac{1}{t}-\int\left(\dfrac{1}{t}\int\dfrac{1}{t}dt\right)dt\\
				& = \log^{2}{t}-\int\dfrac{\log{t}}{t}dt
		\end{align*}
	Thus, we get
		\begin{align*}
			\int\limits_{1}^{x}\dfrac{\log{t}}{t}dt
				& = \dfrac{1}{2}\log^{2}{x}
		\end{align*}
	which in turn gives
		\begin{align*}
			T_{3}(x)
				& = \dfrac{1}{2}x\log^{2}{x}+O(x\log{x})
		\end{align*}
	We leave it as an exercise for the reader to prove the following (from what we have already developed, induction is one way to go about it).
		\begin{theorem}
			Let $k$ be a positive integer. Then
				\begin{align*}
					T_{k}(x)
						& = \dfrac{1}{(k-1)!}x\log^{k-1}{x}+O\left(x\log^{k-2}{x}\right)
				\end{align*}
		\end{theorem}
	The reason we do not write $T_{k}(x)$ as $O\left(x\log^{k-1}{x}\right)$ directly is because in this case, we already know what the constant multiplier of $x\log^{k-1}{x}$ is. Usually, we write $O(f(x))$ when we do not know what the constant multiplier of $f(x)$ is. \textcite[Page $2$]{landau_1912_0} states a sharper result.
		\begin{align*}
			T_{k}(x)
				& = x\left(\sum_{m=0}^{k-1}b_{m}\log^{m}{x}\right)+O\left(x^{1-\frac{1}{k}}\right)+O\left(x^{1-\frac{1}{k}}\log^{k-2}{x}\right)
		\end{align*}
	Let us now turn our attention to improving the asymptotic of $S_{k}(x)$.
		\begin{align*}
			S_{k}(x)
				& = \sum_{n\leq x}\sum_{d\mid n}d^{k}\\
				& = \sum_{n\leq x}\sum_{m\leq x/n}m^{k}\\
				& = \sum_{n\leq x}\mathfrak{S}_{k}\left(\dfrac{x}{n}\right)\\
				& = \sum_{n\leq x}\dfrac{x^{k+1}}{(k+1)n^{k+1}}+O\left(\dfrac{x^{k}}{n^{k}}\right)\\
				& = \dfrac{x^{k+1}}{k+1}\sum_{n\leq x}\dfrac{1}{n^{k+1}}+O\left(x^{k}\sum_{n\leq x}\dfrac{1}{n^{k}}\right)
		\end{align*}
	Here, we can see that the function
		\begin{align*}
			\sum_{n\leq x}\dfrac{1}{n^{k}}
		\end{align*}
	occurs repeatedly. It is in fact, the partial sum of the famous Euler's \index{Zeta function}\textit{zeta function}.
		\begin{definition}[Zeta function]
			For a complex number $s$, the zeta function $\zeta(s)$ is defined as
				\begin{align*}
					\zeta(s)
						& = \sum_{n\geq 1}\dfrac{1}{n^{s}}
				\end{align*}
		\end{definition}
	We will discuss zeta function in details in \autoref{sec:zeta}. For now, let us establish a result similar to \nameref{thm:dirtau} for partial sums of $\zeta$. Setting $f(n):=n^{-s}$ and $a_{n}=1$ in \nameref{thm:abel}, $A(x)=\lfloor{x}\rfloor=x+O(1)$ and
		\begin{align*}
			\sum_{n\leq x}\dfrac{1}{n^{s}}
				& = \lfloor{x}\rfloor x^{-s}-\int\limits_{1}^{x}(t+O(1))f'(t)dt\\
				& = x^{1-s}+O\left(x^{-s}\right)+s\int\limits_{1}^{x}t^{-s}dt+O\left(s\int\limits_{1}^{x}t^{-s-1}dt\right)\\
				& = x^{1-s}+\dfrac{s}{1-s}\left(x^{1-s}-1\right)+O\left(\int\limits_{1}^{x}t^{-s-1}\right)\\
				& = \dfrac{x^{1-s}}{1-s}+C+O(x^{-s})
		\end{align*}
	Similar to $\gamma$, we can take $x\to\infty$ and get the following result.
		\begin{theorem}
			Let $s$ be a positive real number other than $1$. Then
				\begin{align*}
					\sum_{n\leq x}\dfrac{1}{n^{s}}
						& = \dfrac{x^{1-s}}{1-s}+C+O(x^{-s})
				\end{align*}
			where $C$ is a constant similar to Euler-Mascheroni constant dependent on $s$ and
				\begin{align*}
					C
						& = \lim\limits_{x\to\infty}\left(\sum_{n\leq x}\dfrac{1}{n^{s}}-\dfrac{x^{1-s}}{1-s}\right)
				\end{align*}
			Furthermore, if $0<s<1$, then $C=\zeta(s)$ since $x^{1-s}\to 0$.
		\end{theorem}
	We can now get back to estimating $S_{k}(x)$.
		\begin{align*}
			S_{k}(x)
				& = \dfrac{x^{k+1}}{k+1}\sum_{n\leq x}\dfrac{1}{n^{k+1}}+O\left(x^{k}\sum_{n\leq x}\dfrac{1}{n^{k}}\right)\\
				& = \dfrac{x^{k+1}}{k+1}\left(\dfrac{x^{-k}}{-k}+\zeta(k+1)+O(x^{-k-1})\right)+O\left(x^{k}\left(\dfrac{x^{1-k}}{1-k}+\zeta(k)+O(x^{-k})\right)\right)\\
				& = \dfrac{x}{-k(k+1)}+\dfrac{x^{k+1}}{k+1}\zeta(k+1)+O(x^{k+1-k-1})+O\left(\dfrac{x}{1-k}+x^{k}\zeta(k)+O(1)\right)\\
				& = \dfrac{x^{k+1}}{k+1}\zeta(k+1)+O(x)+O(1)+O(x+x^{k})
		\end{align*}
	From this, we finally get the following.
		\begin{theorem}
			Let $k$ be a positive integer. Then
				\begin{align*}
					S_{k}(x)
						& = \dfrac{x^{k+1}}{k+1}\zeta(k+1)+O(x^{\max(1,k)})
				\end{align*}
		\end{theorem}
	We leave the case when $k$ is a negative integer as an exercise. Next, we consider a generalization of the Euler's totient function $\varphi(n)$. We will refrain from establishing the order of Jordan function for now.
		\begin{align*}
			\varphi(x,a)
				& = \sum_{\substack{n\leq x\\\gcd(n,a)=1}}1
		\end{align*}
	For a positive integer $n$, $\varphi(n)=\varphi(n,n)$ and \index{Jordan function}\textit{Jordan function} is a generalization of $\varphi$.
		\begin{definition}[Jordan function]
			Let $n$ and $k$ be positive integers. Then the Jordan function $J_{k}(n)$ is the number of $k$ tuples of positive integers not exceeding $n$ that are relatively prime to $n$.
				\begin{align*}
					J_{k}(n)
						& = \sum_{\substack{1\leq a_{1},\ldots,a_{k}\leq n\\\gcd(a_{1},\ldots,a_{k},n)=1}}1
				\end{align*}
			\textcite{lehmer_1900} used the notation $\varphi_{k}(n)$ but today $J_{k}(n)$ is used more often.
		\end{definition}
	\textcite[Page $95-97$]{jordan_1989} first discussed this function and \textcite{lehmer_1900} developed some asymptotic results. Jordan totient function is interesting not only because it is a generalization of Euler's totient function but also because it has many interesting properties.
	For example, similar to $\varphi$, we can show that
		\begin{align*}
			J_{k}(n)
				& = \prod_{p^{e}\|n}p^{k(e-1)}(p-1)\\
			J_{k}(n^{m})
				& = n^{k(m-1)}J_{k}(n)
		\end{align*}
	\textcite[Theorem VI]{lehmer_1900} proves the following which he calls the \textit{fundamental theorem}.
		\begin{align}
			J_{k}(mn)
				& = J_{k}(n)\prod_{p^{e}\parallel m}\left(p^{ke}-p^{k(e-1)}\lambda(n,p)\right)\label{thm:lehfund}
		\end{align}
	where $\lambda(n,p)=0$ if $p\mid n$ otherwise $\lambda(n,p)=1$. We leave the proof of this result and the following to the reader.
		\begin{align*}
			\sum_{d\mid n}J_{k}(d)
				& = n^{k}
		\end{align*}
	 Like $\sigma_{k}(n)$, $J_{k}(n)$ is also  related to the sum $\mathfrak{S}(x, k)$. But before we further look into $J_{k}(n)$, we will have to discuss M\"{o}bius inversion as well as generalized inversion.
\end{document}