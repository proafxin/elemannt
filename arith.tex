\documentclass[elemannt.tex]{subfile}

\begin{document}
	\chapter{Arithmetic Functions}\label{ch:arith}
	In this chapter, we will discuss some generalized arithmetic functions and their asymptotic behavior. By asymptotic behavior, we mean that we want to understand how a function $f(x)$ grows as $x$ tends to infinity. A common way of analyzing growth of an arithmetic function $f$ is to consider the order of an arithmetic function. \gls{erdos}
		\begin{definition}[Order of Arithmetic Function]
			The order of an arithmetic function $f$ is defined by the asymptotic $\lim_{x\to\infty}f(x)$. To understand the growth of $f$, we often analyze the asymptotic of partial summation
				\begin{align*}
					\lim\limits_{x\to\infty}\sum_{n\leq x}f(n)
				\end{align*}
		\end{definition}
	For example, the prime counting function is
		\begin{align*}
			\pi(x)
				& = \sum_{n\leq x}C(n)
		\end{align*}
	where $C(n)$ is the characteristic function of $n$, that is, $C(n)=1$ if $n$ is a prime otherwise $C(n)=0$. One of the biggest questions we will try to answer is how $\lim_{x\to\infty}\pi(x)$ behaves.
		\begin{definition}[Summatory Function]
			For an arithmetic function $f$, the \index{Summatory function}\textit{summatory function} of $f$ is defined as
				\begin{align*}
					F(n)
						& = \sum_{d\in \mathbb{S}}f(d)
				\end{align*}
			where $\mathbb{S}$ is some set possibly dependent on $n$.
		\end{definition}
	When $\mathbb{S}$ is the set of divisors of $n$, the number of divisor function $\tau(n)$ is the summatory function of the unit function $u(n)=1$ and the sum of divisor function $\sigma(n)$ is the summatory function of the invariant function $f(n)=n$. Another summatory function is the partial summation
		\begin{align*}
			\sum_{n\leq x}f(n)
		\end{align*}
	Associated with this is the average order of $f$.
		\begin{definition}[Average Order]
			 For an arithmetic function $f$,
				\begin{align*}
					\lim\limits_{x\to\infty}\frac{\sum_{n\leq x}f(x)}{x}
				\end{align*}
			is the \index{Average order}\textit{average order}.
		\end{definition}
	In this context, a very interesting way of analyzing growth is the \textit{normal order} of $f$. The concept of normal numbers arises from \textcite{hardy_aiyangar_1917}.
		\begin{definition}[Normal Order]
			Let $f$ and $F$ be arithmetic functions such that
				\begin{align}
					(1-\epsilon)F(n)
						& < f(n)<(1+\epsilon)F(n)\label{eqn:normal}
				\end{align}
			holds for \textit{almost} all $n\leq x$ as $x\to\infty$. Then we say that $F$ is the \index{Normal order}\textit{normal order} of $f$.
		\end{definition}
	A trivial(?) example of normal order is that almost all positive integers not exceeding $x$ are composite if $x$ is sufficiently large. We should probably elaborate on what we mean by \textit{almost} here. One interpretation is that the number of positive integers not exceeding $x$ which are prime is very small compared to $x$. Similarly, $f$ is of order $F$ means that the number of positive integers $n$ not exceeding $x$ which do not satisfy \eqref{eqn:normal} is very small compared to $x$.

	An interesting property in summatory functions is that
		\begin{align*}
			\sum_{i=1}^{n}F(i)
				& = \sum_{i=1}^{n}\sum_{d\mid i}f(d)\\
				& = \sum_{i=1}^{n}\floor{\frac{n}{i}}f(i)
		\end{align*}
	Here, the last equation is true because there are $\lfloor{n/i}\rfloor$ multiples of $i$ not exceeding $n$.
	\section{Order of Some Arithmetic Functions}
	Recall that the number of divisor function
		\begin{align*}
			\tau(n)
				&  =\sum_{ab=n}1
		\end{align*}
	We can generalize this as follows.
		\begin{definition}[Generalized Number of Divisors]
			The \index{Generalized number of divisor}\textit{generalized number of divisor} function is defined as
				\begin{align*}
					\tau_{k}(n)
						& = \sum_{d_{1}\cdots d_{k}=n}1
				\end{align*}
			So $\tau_{k}(n)$ is the number of ways to write $n$ as a product of $k$ positive integers.
		\end{definition}
	Similarly, we can take the sum of divisor function and generalize it.
		\begin{definition}[Generalized Sum of Divisors]
			The \index{Generalized sum of divisor}\textit{generalized sum of divisor} function can be defined as
				\begin{align*}
					\sigma_{k}(n)
						& = \sum_{d\mid n}d^{k}
				\end{align*}
		\end{definition}
	At this point, we should discuss some asymptotic notions.
		\begin{definition}[Big O]
			Let $f$ and $g$ be two real or complex valued functions. We say that
				\begin{align*}
					f(x)
					& = O(g(x))
				\end{align*}
			if there is a positive real constant $C$ such that
				\begin{align*}
					|f(x)|
					& \leq Cg(x)
				\end{align*}
			for all sufficiently large $x$. It is also written as $f(x)\ll g(x)$  or $g(x)\gg f(x)$. When we say $g$ is an asymptotic estimate of $f$, we mean that
				\begin{align*}
					f(x)
						& = g(x)+O(h(x))
				\end{align*}
			for two functions $g$ and $h$ as $x\to\infty$. Here, $h$ is the \textit{error term} which obviously should be of lower magnitude than $g$.
		\end{definition}
	In particular, $f(x)=O(1)$ means that $f$ is bounded above by some positive constant. Some trivial examples are $x^{2}=O(x^{3})$, $x+1=O(x)$ and $x^{2}+2x=O(x^{2})$. We usually want $g(x)$ to be as small as possible to avoid triviality. A useful example is
		\begin{align*}
			\left\lfloor{x}\right\rfloor
				& = x+O(1)
		\end{align*}
	since $x=\lfloor{x}\rfloor+\{x\}$ and $0\leq \{x\}<1$.
		\begin{definition}[Small O]
			Let $f$ and $g$ be two real or complex valued functions. Then the following two statements are equivalent
				\begin{align}
					f(x)
						& = o(g(x))\\
					\lim\limits_{x\to\infty}\frac{f(x)}{g(x)}
						& = 0
				\end{align}
		\end{definition}
	Some trivial examples are $1/x=o(1)$, $x=o(x^{2})$ and $2x^{2}\neq o(x^{2})$.  \textcite[Page 883 (second volume is paged consecutively after first volume)]{landau_1909} states that the symbol $O$ had been first used by \textcite[Page 401]{bachmann_1894}. \textcite{hardy_1910} uses the notations $\prec$ and $\succ$ respectively but they are no longer in practice. \textcite{hardy_riesz_1915} adopted the notations small $o$ and big $O$ and today these are the primary notations for this purpose.

	It should be evident that having an estimate with respect to $O$ asymptotic formulas is more desirable than $o$ formulas. By nature, $O$ formulas give us a better understanding and a specific estimate whereas $o$ does not always say as much. Moreover, working with $O$ is a lot easier than working with $o$. For example,
		\begin{align*}
			\sum O(f(x))
				& = O\parenthesis{\sum f(x)}\\
			\int O(f(x))dx
				& = O\parenthesis{\int f(x)dx}
		\end{align*}
	Or consider the possibility that we can very easily deal with constants that would otherwise pop up here and there unnecessarily. With the help of $O$,
		\begin{align*}
			O(1)+c
				& = O(1)\\
			O(cf(x))
				& = O(f(x))
		\end{align*}
	and so on.
		\begin{definition}[Equivalence]
			Let $f$ and $g$ be two real or complex valued functions. We say that they are \textit{asymptotically equivalent} if
				\begin{align*}
					\lim\limits_{x\to\infty}\frac{f(x)}{g(x)}
						& = 1
				\end{align*}
			and we denote it by $f\sim g$. So, we can say that $g$ is an asymptotic formula for $f$.
		\end{definition}
	An example is $x^{2}\sim x^{2}+x$. Another example in connection with normal order is that $f$ has normal order $F$ if the number of $n$ not satisfying \eqref{eqn:normal} is $o(x)$. We can also say, the number of $n$ satisfying \eqref{eqn:normal} is $\sim x$. Note the following.
		\begin{align*}
			f \sim g
				& \iff |f(x)-g(x)|= o(g(x))
		\end{align*}
	We will use these symbols extensively throughout the book. It is of utmost importance that the reader gets well familiarized with these notions since they will be crucial in understanding much of this book. The primary motivation behind these asymptotic notions is to get an as precise as possible idea about the \textit{order of magnitude} of a certain function. This is why we will be leaning more towards $x^{2}+2x=O(x^{2})$ than $x^{2}+2x=O(x^{3})$ even though both are mathematically correct. The reason is, even though $x^{2}+2x=O(x^{3})$ is true, it is taking away a great portion of the accuracy to which we suppose $x^{2}+2x$ should be measured with. On the other hand, we easily see that we cannot have $x^{2}+2x=O(x^{\epsilon})$ for $\epsilon<2$. Under the same philosophy, we define the order of magnitude equivalence.
		\begin{definition}
			If $f$ and $g$ are functions such that both $f(x)\ll g(x)$ and $g(x)\ll f(x)$ hold, then we write $f\asymp g$ and say that $f$ and $g$ have the \textit{same order of magnitude}.
		\end{definition}\par
	Now, we are interested in the order of general number of divisors and general sum of divisors. Let us define the cumulative sum of these functions.
		\begin{align*}
			S_{k}(x)
				& = \sum_{n\leq x}\sigma_{k}(n)\\
			T_{k}(x)
				& = \sum_{n\leq x}\tau_{k}(n)
		\end{align*}
	Notice the following.
		\begin{align*}
			S_{k}(x)
				& = \sum_{n\leq x}\sum_{d\mid n}d^{k}\\
				& = \sum_{n\leq x}\floor{\frac{x}{n}} n^{k}\\
				& = \sum_{n\leq x}\parenthesis{\frac{x}{n}+O(1)}n^{k}\\
				& = x\sum_{n\leq x}n^{k-1}+O\parenthesis{\sum_{n\leq x}n^{k}}
		\end{align*}
	We can use this to establish an asymptotic for $T_{k}(x)$ if we can establish the asymptotic of $A_{2}(x)$. We will get to that in a moment. First, let us take care of the summation within the big O bracket. We have the trivial inequality that
		\begin{align*}
			\sum_{n\leq x}n^{k}
				& \leq \sum_{n\leq x}x^{k}\\
				& = x^{k}\sum_{n\leq x}1\\
				& = \floor{x} x^{k}\\
				& = (x+O(1))x^{k}\\
				& = x^{k+1}+O(x^{k})
		\end{align*}
	We have that $S_{k}(x)=x\parenthesis{x^{k}+O(x^{k-1})}+O(x^{k+1})=O(x^{k+1})$. Although weak, we get an estimate this way. On this note, an interested reader can try and prove that
		\begin{align*}
			(n+1)^{k+1}-1
				& =\sum_{i=0}^{k}\binom{k+1}{i}\mathfrak{S}(n,i)
		\end{align*}
	where $\mathfrak{S}(x,k)=\sum_{n\leq x}n^{k}$. This is known as the \index{Pascal identity}\textit{Pascal identity} (see \textcite{pascal_1964}, for an English translation, see \textcite{knoebel_laubenbacher_lodder_pengelley_2007}). \textcite[Chapter II, Theorem $1$]{lehmer_1900} proves that
		\begin{align}
			\mathfrak{S}(x,k)
				& = \frac{x^{k+1}}{k+1}+\Delta\label{eqn:lehmers}
		\end{align}
	where $|\Delta|\leq x^{k}$. The reader may also be interested in \textcite{kieren_macmillan_jonathan_sondow_2011}.

	We shall try to estimate $T$ in a similar fashion. First, see that
		\begin{align*}
			\tau_{k}(n)
				& = \sum_{d_{1}\cdots d_{k}=n}1\\
				& = \sum_{d_{k}\mid n}\sum_{d_{1}\cdots d_{k-1}=n/d_{k}}1\\
				& = \sum_{d\mid n}\tau_{k-1}\parenthesis{\frac{n}{d}}
		\end{align*}
	Note that the two sets $\{d:d\mid n\}$ and $\{n/d:d\mid n\}$ are actually the same. So, we get
		\begin{align*}
			\tau_{k}(n)
				& = \sum_{d\mid n}\tau_{k-1}(d)
		\end{align*}
	\textcite[($\S$8)]{beumer_1962} also considers the generalization $\tau_{k}(n)$ in this exact form. Using this for $T$,
		\begin{align*}
			T_{k}(x)
				& = \sum_{n\leq x}\tau_{k}(n)\\
				& = \sum_{n\leq x}\sum_{d\mid n}\tau_{k-1}(d)\\
				& = \sum_{n\leq x}\floor{\frac{x}{n}}\tau_{k-1}(n)\\
				& = \sum_{n\leq x}\parenthesis{\frac{x}{n}+O(1)}\tau_{k-1}(n)\\
				& = x\sum_{n\leq x}\frac{\tau_{k-1}(n)}{n}+O\parenthesis{\sum_{n\leq x}\tau_{k-1}(n)}
		\end{align*}
	Thus, we have the recursive result
		\begin{align*}
			T_{k}(x)
				& = x\sum_{n\leq x}\frac{\tau_{k-1}(n)}{n}+O(T_{k-1}(x))
		\end{align*}
	It gets nontrivial how to proceed from here. Consider the \textit{harmonic sum}
		\begin{align*}
			H(x)
			& = \sum_{n\leq x}\frac{1}{n}
		\end{align*}
	It does not seem easy to calculate $H$ accurately, however, we can make a decent attempt to estimate $H$. The tool that is best suited for carrying out such an estimation is the \index{Abel partial summation formula}\textit{Abel partial summation formula}. \textcite{abel_1826} states this formula which today is a cornerstone of analytic number theory.
		\begin{theorem}[Abel partial summation formula]\label{thm:abel}
			Let $\{a_{n}\}$ be a sequence of real numbers and $f$ be a continuous differentiable function in $[y,x]$. If the partial sums of $\{a_{n}\}$ is
				\begin{align*}
					A(x)
						& = \sum_{n\leq x}a_{n}
				\end{align*}
			are known, then
				\begin{align*}
					\sum_{y<n\leq x}a_{n}f(n)
						& = A(x)f(x)-A(y)f(y)-\int\limits_{y}^{x}A(t)f'(t)dt
				\end{align*}
			In particular, if $f$ is an arithmetic function,
				\begin{align*}
					\sum_{n\leq x}a_{n}f(n)
						& = A(x)f(x)-\int\limits_{1}^{x}A(t)f'(t)dt
				\end{align*}
		\end{theorem}

		\begin{proof}

		\end{proof}
	It is not straightforward to realize how such a formula can be as influential as we are describing it to be. Notice that, the formula essentially converts a discreet sum into an integral, which occasionally may be calculable. If the integral is not calculable, we may be able to estimate its value sometimes. We should mention that \textcite[Page $83$, $\S4$]{aiyangar_hardy_vennkatesvara_seshu_aiyar_p_wilson_1927} also uses a method that can only be described as the partial summation formula. It is unclear if Ramanujan simply knew about this. He essentially derives the partial summation formula while trying to express a sum of the form
		\begin{align*}
			\sum_{p\leq x}\phi(p)
		\end{align*}
	with respect to $\pi(x),\phi(x)$ and an integral where $\pi(x)$ is the number of primes not exceeding $x$. A consequence of \nameref{thm:abel} is the celebrated \index{Euler's summation formula}\textit{Euler's summation formula}.
		\begin{theorem}[Euler's summation formula]\label{thm:eulersum}
			Let $f$ be a continuous differentiable function in $[y,x]$. Then
				\begin{align*}
					\sum_{y<n\leq x}f(n)
						& = \int\limits_{y}^{x}f(t)dt+\int\limits_{y}^{x}\{t\}f(t)dt+\{y\}f(y)-\{x\}f(x)
				\end{align*}
			where $\{t\}=t-\lfloor{t}\rfloor$ is the fractional part of $t$.
		\end{theorem}

		\begin{proof}

		\end{proof}
	As an application of \nameref{thm:eulersum}, we can derive a result similar to \eqref{eqn:lehmers} taking  $f(n)=n^{k}$ for $k\geq 0$.
		\begin{align*}
			\mathfrak{S}_{k}(x)
				& = \sum_{n\leq x}n^{k}\\
				& = \int\limits_{1}^{x}t^{k}dt+k\int\limits_{1}^{x}t^{k-1}(t-\floor{t})dt+1-(x-\floor{x})x^{k}\\
				& = \frac{x^{k+1}}{k+1}-\frac{1}{k+1}+O\parenthesis{k\int\limits_{1}^{x}t^{k-1}dt}+O(x^{k})\\
				& = \frac{x^{k+1}}{k+1}+O(x^{k})
		\end{align*}
	Setting $a_{n}=\tau_{k-1}(n)$ and $f(n)=1/n$ in \nameref{thm:abel}, we get
		\begin{align*}
			\sum_{n\leq x}\frac{\tau_{k-1}(n)}{n}
				& = \frac{T_{k-1}(x)}{x}-\int\limits_{1}^{x}-\frac{T_{k-1}(t)}{t^{2}}dt
		\end{align*}
	Thus, we have a result where we can inductively get to the final expression. First, let us see the case $k=2$.
		\begin{align*}
			\sum_{n\leq x}\tau(n)
				& = \sum_{n\leq x}\floor{\frac{x}{n}}
		\end{align*}
	Clearly, this is just the number of pairs $(a,b)$ such that $ab\leq x$. We can divide the pairs in two classes. In the first class, $1\leq a\leq \sqrt{x}$ and in the second one, $a>\sqrt{x}$. In the first case, for a fixed $a$, there are $\lfloor{x/a}\rfloor$ possible choices for a valid value of $b$. So, the number of pairs in the first case is
		\begin{align*}
			\sum_{a\leq \sqrt{x}}\floor{\frac{x}{a}}
		\end{align*}
	In the second case, since $a>\sqrt{x}$ and $b\leq x/a$, we must have $b\leq \sqrt{x}$. For a fixed $b$, there are $\lfloor{x/b}\rfloor-\sqrt{x}$ choices for a valid value of $a$, the choices namely are
		\begin{align*}
			\lfloor{x}\rfloor+1,\ldots,\floor{\frac{x}{b}}
		\end{align*}
	Then the number of pairs in this case is
		\begin{align*}
			\sum_{b\leq \sqrt{x}}\floor{\frac{x}{b}}-\floor{\sqrt{x}}
		\end{align*}
	Thus, the total number of such pairs is
		\begin{align}
			\sum_{a\leq \sqrt{x}}\floor{\frac{x}{a}}+\sum_{b\leq \sqrt{x}}\parenthesis{\floor{\frac{x}{b}}-\floor{\sqrt{x}}}
				& = 2\sum_{n\leq \sqrt{x}}\floor{\frac{x}{n}}-\floor{x}^{2}\label{eqn:totdiv}
		\end{align}
	For getting past this sum, we have to deal with the sum
		\begin{align*}
			\sum_{n\leq \sqrt{x}}\floor{x/n}
				& = \sum_{n\leq \sqrt{x}}\parenthesis{\frac{x}{n}+O(1)}\\
				& = x\sum_{n\leq \sqrt{x}}\frac{1}{n}+O(\sqrt{x})\\
				& = xH(\sqrt{x})+O(\sqrt{x})
		\end{align*}
	Setting $a_{n}=1$ and $f(n)=1/n$ in \nameref{thm:abel}, we get
		\begin{align*}
			H(x)
				& = \frac{A(x)}{x}-\int\limits_{1}^{x}-\frac{A(t)}{t^{2}}dt
		\end{align*}
	Here, $A(x)=\lfloor{x}\rfloor=x+O(1)$. Using this,
		\begin{align*}
			H(x)
				& = 1+O\parenthesis{\frac{1}{x}}+\int\limits_{1}^{x}\parenthesis{\frac{1}{t}+\frac{O(1)}{t^{2}}}dt\\
				& = 1+O\parenthesis{\frac{1}{x}}+\int\limits_{1}^{x}\frac{1}{t}dt+O\parenthesis{\int\limits_{1}^{x}\frac{1}{t^{2}}dt}\\
				& = 1+O\parenthesis{\frac{1}{x}}+\log{x}+O\parenthesis{1-\frac{1}{x}}
		\end{align*}
	Thus, we have the following result.
		\begin{theorem}[Divergence of Harmonic Sum]\label{thm:harmonicsum}
			For $x\geq 1$,
				\begin{align*}
					H(x)
						& = \log{x}+C+O\parenthesis{\frac{1}{x}}
				\end{align*}
			where $C$ is a constant.
		\end{theorem}
	We get a more precise formulation of $H(x)$ by considering the limit $x\to\infty$ which removes $O(1/x)$ from the expression since this limit would be $0$.
		\begin{theorem}
			There is a constant $\gamma$ such that
				\begin{align*}
					\gamma
						& = \lim\limits_{x\to\infty}(H(x)-\log{x})
				\end{align*}
		\end{theorem}
	This constant $\gamma$ is now known as \index{Euler-Mascheroni constant}\textit{Euler's constant} or \textit{Euler-Mascheroni's constant}, although, neither Euler nor Mascheroni used the notation $\gamma$ for this constant. \textcite{euler_1740} (republished in \textcite{euler_2020}) used $C$ and $O$ in his original paper. \textcite{mascheroni_1790} used $A$ and $a$. Today it is not known whether $\gamma$ is even irrational. For now, we will not require the use of $\gamma$, so we will use \nameref{thm:harmonicsum}. Applying this, we have
		\begin{align*}
			\sum_{n\leq \sqrt{x}}\floor{\frac{x}{n}}
				& = xH(\sqrt{x})+O(\sqrt{x})\\
				& = x\parenthesis{C+\log{\sqrt{x}}+O\parenthesis{\frac{1}{\sqrt{x}}}}+O(\sqrt{x})\\
				& = \frac{1}{2}x\log{x}+Cx+O\parenthesis{\frac{x}{\sqrt{x}}}+O(\sqrt{x})\\
				& = \frac{1}{2}x\log{x}+O(x)
		\end{align*}
	We can now use this to get
		\begin{align*}
			\sum_{n\leq x}\tau(n)
				& = 2\sum_{n\leq \sqrt{x}}\floor{x/n}-\floor{\sqrt{x}}^{2}\\
				& = x\log{x}+O(x)
		\end{align*}
	Thus, we get the following result.
		\begin{align*}
			\frac{\sum_{n\leq x}\tau(n)}{x}
				& = \log{x}+O(1)
		\end{align*}
	\textcite{dirichlet_1897} actually proves the more precise result given below.
		\begin{theorem}[Dirichlet's average order of $\tau$ theorem]\label{thm:dirtau}
				\begin{align*}
					\frac{\sum_{n\leq x}\tau(n)}{x}
						& = \log{x}+2\gamma-1+\bigo{\frac{1}{\sqrt{x}}}
				\end{align*}
			where $\gamma$ is the Euler-Mascheroni constant.
		\end{theorem}
	Then Dirichlet's theorem on $\tau$ can be restated as \textit{the average order of $\tau$ is }$O(\log{x})$. \textcite{aiyangar_hardy_vennkatesvara_seshu_aiyar_p_wilson_1927} points out in his paper that the error term $O(1/\sqrt{x})$ in Dirichlet's theorem can be improved to $\bigo{x^{-2/3+\epsilon}}$ or $\bigo{x^{-2/3}\log{x}}$ as \textcite[Page $689$]{landau_1912} shows.

	We can now get back to estimating $T$. Using \nameref{thm:abel}, we were able to deduce
		\begin{align*}
			T_{k}(x)
				& = \bigo{T_{k-1}(x)}+x\int\limits_{1}^{x}\frac{T_{k-1}(t)}{t^{2}}dt
		\end{align*}
	Using \nameref{thm:dirtau}, $T(x)=x\log{x}+O(x)$, so
		\begin{align*}
			T_{3}(x)
				& = O(T(x))+x\int\limits_{1}^{x}\frac{T(t)}{t^{2}}dt\\
				& = O(x\log{x})+x\int\limits_{1}^{x}\frac{\log{t}+O(1)}{t}dt\\
				& = O(x\log{x})+x\int\limits_{1}^{x}\frac{\log{t}}{t}dt+x\bigo{\int\limits_{1}^{x}\frac{1}{t}dt}\\
				& = O(x\log{x})+x\int\limits_{1}^{x}\frac{\log{t}}{t}dt
		\end{align*}
	Using integration by parts,
		\begin{align*}
			\int\frac{\log{t}}{t}dt
				& = \log{t}\int\frac{1}{t}-\int\parenthesis{\frac{1}{t}\int\frac{1}{t}dt}dt\\
				& = \log^{2}{t}-\int\frac{\log{t}}{t}dt
		\end{align*}
	Thus, we get
		\begin{align*}
			\int\limits_{1}^{x}\frac{\log{t}}{t}dt
				& = \frac{1}{2}\log^{2}{x}
		\end{align*}
	which in turn gives
		\begin{align*}
			T_{3}(x)
				& = \frac{1}{2}x\log^{2}{x}+O(x\log{x})
		\end{align*}
	We leave it as an exercise for the reader to prove the following (from what we have already developed, induction is one way to go about it).
		\begin{theorem}
			Let $k$ be a positive integer. Then
				\begin{align*}
					T_{k}(x)
						& = \frac{1}{(k-1)!}x\log^{k-1}{x}+\bigo{x\log^{k-2}{x}}
				\end{align*}
		\end{theorem}
	The reason we do not write $T_{k}(x)$ as $\bigo{x\log^{k-1}{x}}$ directly is because in this case, we already know the constant multiplier of $x\log^{k-1}{x}$ which is not ugly. Usually, we write $O(f(x))$ when we do not know what the constant multiplier of $f(x)$ is or when it gets too big to keep track of. \textcite[Page $2$]{landau_1912_0} states a sharper result.
		\begin{align*}
			T_{k}(x)
				& = x\parenthesis{\sum_{m=0}^{k-1}b_{m}\log^{m}{x}}+\bigo{x^{1-\frac{1}{k}}}+\bigo{x^{1-\frac{1}{k}}\log^{k-2}{x}}
		\end{align*}
	Let us now turn our attention to improving the asymptotic of $S_{k}(x)$.
		\begin{align*}
			S_{k}(x)
				& = \sum_{n\leq x}\sum_{d\mid n}d^{k}\\
				& = \sum_{n\leq x}\sum_{m\leq x/n}m^{k}\\
				& = \sum_{n\leq x}\mathfrak{S}_{k}\parenthesis{\frac{x}{n}}\\
				& = \sum_{n\leq x}\frac{x^{k+1}}{(k+1)n^{k+1}}+\bigo{\frac{x^{k}}{n^{k}}}\\
				& = \frac{x^{k+1}}{k+1}\sum_{n\leq x}\frac{1}{n^{k+1}}+\bigo{x^{k}\sum_{n\leq x}\frac{1}{n^{k}}}
		\end{align*}
	Here, we can see that the function
		\begin{align*}
			\sum_{n\leq x}\frac{1}{n^{k}}
		\end{align*}
	occurs repeatedly. It is in fact, the partial sum of the famous Euler's \index{Zeta function}\textit{zeta function}.
		\begin{definition}[Zeta Function]
			For a complex number $s$, the zeta function $\zeta(s)$ is defined as
				\begin{align*}
					\zeta(s)
						& = \sum_{n\geq 1}\frac{1}{n^{s}}
				\end{align*}
		\end{definition}
	We will discuss zeta function in details in \autoref{sec:zeta}. For now, let us establish a result similar to \nameref{thm:dirtau} for partial sums of $\zeta$. Setting $f(n):=n^{-s}$ and $a_{n}=1$ in \nameref{thm:abel}, $A(x)=\floor{x}=x+O(1)$ and
		\begin{align*}
			\sum_{n\leq x}\frac{1}{n^{s}}
				& = \floor{x}x^{-s}-\int\limits_{1}^{x}(t+O(1))f'(t)dt\\
				& = x^{1-s}+\bigo{x^{-s}}+s\int\limits_{1}^{x}t^{-s}dt+\bigo{s\int\limits_{1}^{x}t^{-s-1}dt}\\
				& = x^{1-s}+\frac{s}{1-s}\parenthesis{x^{1-s}-1}+\bigo{\int\limits_{1}^{x}t^{-s-1}dt}\\
				& = \frac{x^{1-s}}{1-s}+C+O(x^{-s})
		\end{align*}
	Similar to $\gamma$, we can take $x\to\infty$ and get the following result.
		\begin{theorem}
			Let $s$ be a positive real number other than $1$. Then
				\begin{align*}
					\sum_{n\leq x}\frac{1}{n^{s}}
						& = \frac{x^{1-s}}{1-s}+C+O(x^{-s})
				\end{align*}
			where $C$ is a constant similar to Euler-Mascheroni constant dependent on $s$ and
				\begin{align*}
					C
						& = \lim\limits_{x\to\infty}\parenthesis{\sum_{n\leq x}\frac{1}{n^{s}}-\frac{x^{1-s}}{1-s}}
				\end{align*}
			Furthermore, if $0<s<1$, then $C=\zeta(s)$ since $x^{1-s}\to 0$.
		\end{theorem}
	We can now get back to estimating $S_{k}(x)$.
		\begin{align*}
			S_{k}(x)
				& = \frac{x^{k+1}}{k+1}\sum_{n\leq x}\frac{1}{n^{k+1}}+\bigo{x^{k}\sum_{n\leq x}\frac{1}{n^{k}}}\\
				& = \frac{x^{k+1}}{k+1}\parenthesis{\frac{x^{-k}}{-k}+\zeta(k+1)+O(x^{-k-1})}+\bigo{x^{k}\parenthesis{\frac{x^{1-k}}{1-k}+\zeta(k)+O(x^{-k})}}\\
				& = \frac{x}{-k(k+1)}+\frac{x^{k+1}}{k+1}\zeta(k+1)+O(x^{k+1-k-1})+\parenthesis{\frac{x}{1-k}+x^{k}\zeta(k)+O(1)}\\
				& = \frac{x^{k+1}}{k+1}\zeta(k+1)+O(x)+O(1)+O(x+x^{k})
		\end{align*}
	From this, we finally get the following.
		\begin{theorem}
			Let $k$ be a positive integer. Then
				\begin{align*}
					S_{k}(x)
						& = \frac{x^{k+1}}{k+1}\zeta(k+1)+O(x^{\max(1,k)})
				\end{align*}
		\end{theorem}
	We leave the case when $k$ is a negative integer as an exercise. Next, we consider a generalization of the Euler's totient function $\varphi(n)$.
		\begin{align*}
			\varphi(x,a)
				& = \sum_{\substack{n\leq x\\\gcd(n,a)=1}}1
		\end{align*}
	For a positive integer $n$, $\varphi(n)=\varphi(n,n)$ and \index{Jordan function}\textit{Jordan function} is a generalization of $\varphi$.
		\begin{definition}[Jordan Function]
			Let $n$ and $k$ be positive integers. Then the Jordan function $J_{k}(n)$ is the number of $k$ tuples of positive integers not exceeding $n$ that are relatively prime to $n$.
				\begin{align*}
					J_{k}(n)
						& = \sum_{\substack{1\leq a_{1},\ldots,a_{k}\leq n\\\gcd(a_{1},\ldots,a_{k},n)=1}}1
				\end{align*}
			\textcite{lehmer_1900} used the notation $\varphi_{k}(n)$ but today $J_{k}(n)$ is used more often.
		\end{definition}
	\textcite[Page $95-97$]{jordan_1989} first discussed this function and \textcite{lehmer_1900} developed some asymptotic results. Jordan totient function is interesting not only because it is a generalization of Euler's totient function but also because it has many interesting properties.
	For example, similar to $\varphi$, we can show that
		\begin{align*}
			J_{k}(n)
				& = \prod_{p^{e}\|n}p^{k(e-1)}(p-1)\\
			J_{k}(n^{m})
				& = n^{k(m-1)}J_{k}(n)
		\end{align*}
	\textcite[Theorem VI]{lehmer_1900} proves the following which he calls the \textit{fundamental theorem}.
		\begin{align}
			J_{k}(mn)
				& = J_{k}(n)\prod_{p^{e}\parallel m}\parenthesis{p^{ke}-p^{k(e-1)}\lambda(n,p)}\label{thm:lehfund}
		\end{align}
	where $\lambda(n,p)=0$ if $p\mid n$ otherwise $\lambda(n,p)=1$. We leave the proof of this result and the following to the reader.
		\begin{align}
			\sum_{d\mid n}J_{k}(d)
				& = n^{k}\label{eqn:jordansummatory}
		\end{align}
	 Like $\sigma_{k}(n)$, $J_{k}(n)$ is also  related to the sum $\mathfrak{S}(x, k)$. But we do not derive the order of $J_{k}(n)$ yet.
\end{document}